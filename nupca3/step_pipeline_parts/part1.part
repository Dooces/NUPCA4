"""
nupca3/step_pipeline.py

Axiom-faithful step orchestrator for the 2025-12-21 snapshot.

This implementation is written to be safe to drop into the snapshot:
- No imports of symbols that do not exist in-tree.
- commit_gate() is called with the correct contract (c: list[float]).
- rest_permitted_prev is computed from the actual A14.6 predicate.
- Exports `step_pipeline` as expected by nupca3/agent.py.

Running/compiling is secondary for this repo, but this file avoids the
hard integration failures that otherwise prevent import or immediate execution.
"""

from __future__ import annotations

# =============================================================================
# Debugging
# =============================================================================
# Single switch for step-by-step tracing. When False, the file behaves identically
# except for added comments/docstrings.
DEBUG = True
_LOG_START_TIME = None
_LAST_LOG_TIME = None
def _dbg(msg: str, *, state=None) -> None:
    """Print a debug line when DEBUG is enabled. Does not affect control flow."""
    if not DEBUG:
        return
    bracket = msg.find("]")
    tail = msg[bracket + 1 :] if bracket >= 0 else msg
    if "=" not in tail:
        return
    global _LAST_LOG_TIME, _LOG_START_TIME
    try:
        t = getattr(state, 't', None) if state is not None else None
    except Exception:
        t = None
    now = time.monotonic()
    if _LOG_START_TIME is None:
        _LOG_START_TIME = now
    if _LAST_LOG_TIME is None:
        delta = 0.0
    else:
        delta = now - _LAST_LOG_TIME
    _LAST_LOG_TIME = now
    elapsed = now - _LOG_START_TIME
    prefix = (
        f"[step_pipeline t={int(t):6d} dt={delta:7.3f}s elapsed={elapsed:7.3f}s] "
        if t is not None
        else f"[step_pipeline dt={delta:7.3f}s elapsed={elapsed:7.3f}s] "
    )
    print(prefix + str(msg))

from collections import deque, defaultdict
from dataclasses import asdict, dataclass, field
from typing import Any, Dict, Iterable, List, Set, Tuple

import math
import numpy as np
import time

from .config import AgentConfig
from .types import (
    AgentState,
    EnvObs,
    LearningCache,
    StepTrace,
    Margins,
    PersistentResidualState,
    FootprintResidualStats,
    TransitionRecord,
    WorldHypothesis,
)

from .control.budget import compute_budget_and_horizon
from .control.commitment import commit_gate, select_action
from .control.edit_control import freeze_predicate, permit_param_updates

from .diagnostics.metrics import compute_feel_proxy

from .dynamics.margin_dynamics import HardState, step_hard_dynamics

from .edits.rest_processor import process_struct_queue, RestProcessingResult
from .edits.proposals import propose_structural_edits

from .geometry.fovea import (
    block_of_dim,
    block_slices,
    dims_for_block,
    make_observation_set,
    select_fovea,
    update_fovea_tracking,
    update_fovea_routing_scores,
)
from .geometry.streams import (
    apply_transport,
    coarse_bin_count,
    compute_grid_shift,
    compute_transport_shift,
    extract_coarse,
    grid_cell_mass,
    periph_block_size,
)
from .geometry.binding import build_binding_maps, select_best_binding, select_best_binding_by_fit

from .memory.completion import complete
from .memory.expert import sgd_update
from .memory.fusion import fuse_predictions
from .memory.rollout import rollout_and_confidence
from .memory.salience import compute_salience, get_stress_signals, infer_node_band_level
from .memory.working_set import select_working_set

from .state.baselines import (
    commit_tilde_prev,
    normalize_margins,
    update_baselines,
)
from .state.macrostate import evolve_macrostate, rest_permitted
from .state.margins import compute_arousal, compute_margins, compute_stress, init_margins
from .state.stability import update_stability_metrics


# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------

def _cfg_D(state: AgentState, cfg: AgentConfig) -> int:
    """
    Determine the working dimensionality D used for bounds checking and resizing.

    Inputs:
      state: current AgentState (may have buffer.x_last as a dense vector)
      cfg: AgentConfig (may have cfg.D)

    Output:
      D: int dimensionality used in this step
    """
    try:
        return int(getattr(cfg, "D"))
    except Exception:
        x = getattr(getattr(state, "buffer", None), "x_last", None)
        return int(len(x)) if x is not None else 0


def _filter_cue_to_Oreq(
    cue: Dict[int, float],
    O_req: Set[int],
    D: int,
) -> Dict[int, float]:
    """
    A16.5 masking contract: only dims in O_req are treated as observed, and only
    those within [0, D) are allowed through.

    Inputs:
      cue: sparse observation map {dim -> value}
      O_req: requested observation dims (from fovea blocks)
      D: global dimensionality for bounds

    Output:
      filtered cue dict (subset of cue)
    """
    if not cue:
        return {}
    out: Dict[int, float] = {}
    for k, v in cue.items():
        kk = int(k)
        if 0 <= kk < D and kk in O_req:
            out[kk] = float(v)
    return out


def _ensure_node_band_levels(state: AgentState, cfg: AgentConfig) -> None:
    library = getattr(state, "library", None)
    if library is None:
        return
    nodes = getattr(library, "nodes", {})
    node_levels = getattr(state, "node_band_levels", {})
    seen: Set[int] = set()
    updated = False
    for nid, node in nodes.items():
        key = int(nid)
        seen.add(key)
        if key in node_levels:
            continue
        node_levels[key] = infer_node_band_level(node, cfg)
        updated = True
    stale = [nid for nid in node_levels if nid not in seen]
    for nid in stale:
        node_levels.pop(nid, None)
        updated = True
    if updated:
        state.node_band_levels = node_levels


def _compute_peripheral_gist(x_vec: np.ndarray, cfg: AgentConfig) -> np.ndarray:
    gist = extract_coarse(x_vec, cfg)
    if gist.size > 0:
        return gist
    data = np.asarray(x_vec, dtype=float).reshape(-1)
    if data.size == 0:
        return np.zeros(0, dtype=float)
    stats = np.array(
        [
            float(np.mean(data)),
            float(np.std(data)),
            float(np.min(data)),
            float(np.max(data)),
        ],
        dtype=float,
    )
    return np.nan_to_num(stats)


def _update_context_register(state: AgentState, gist: np.ndarray, cfg: AgentConfig) -> None:
    beta = float(cfg.beta_context)
    beta = max(0.0, min(1.0, beta))
    prev = getattr(state, "context_register", None)
    if prev is None:
        prev = np.zeros_like(gist)
    if prev.shape != gist.shape:
        prev = np.zeros_like(gist)
    new_reg = (1.0 - beta) * prev + beta * gist
    state.context_register = new_reg


def _update_context_tags(state: AgentState, cfg: AgentConfig) -> None:
    beta_tag = float(cfg.beta_context_node)
    beta_tag = max(0.0, min(1.0, beta_tag))
    if beta_tag <= 0.0:
        return
    gist = getattr(state, "context_register", np.zeros(0, dtype=float)).reshape(-1)
    if gist.size == 0:
        return
    tags = getattr(state, "node_context_tags", {})
    for nid in getattr(state, "active_set", set()):
        prev_tag = tags.get(nid)
        if prev_tag is None or prev_tag.shape != gist.shape:
            prev_tag = np.zeros_like(gist)
        tags[nid] = (1.0 - beta_tag) * prev_tag + beta_tag * gist
    state.node_context_tags = tags


def _update_coverage_debts(state: AgentState, cfg: AgentConfig) -> None:
    library = getattr(state, "library", None)
    if library is None:
        return
    nodes = getattr(library, "nodes", {})
    expert_debt = getattr(state, "coverage_expert_debt", {})
    band_debt = getattr(state, "coverage_band_debt", {})
    node_levels = getattr(state, "node_band_levels", {})
    active_set = getattr(state, "active_set", set())

    seen_nodes: Set[int] = set()
    for raw_nid, node in nodes.items():
        nid = int(raw_nid)
        seen_nodes.add(nid)
        level = node_levels.get(nid)
        if level is None:
            level = infer_node_band_level(node, cfg)
            node_levels[nid] = level
        if nid in active_set:
            expert_debt[nid] = 0
        else:
            expert_debt[nid] = expert_debt.get(nid, 0) + 1

    stale_nodes = [nid for nid in expert_debt if nid not in seen_nodes]
    for nid in stale_nodes:
        expert_debt.pop(nid, None)
    stale_levels = [nid for nid in node_levels if nid not in seen_nodes]
    for nid in stale_levels:
        node_levels.pop(nid, None)

    innovation_weight = float(getattr(cfg, "fovea_innovation_weight", 0.0))
    if innovation_weight > 0.0:
        innovation = np.asarray(getattr(state.fovea, "block_innovation", np.zeros(int(getattr(cfg, "B", 0)))), dtype=float)
        incumbents = getattr(state, "incumbents", {})
        for block_id, nodes_in_block in incumbents.items():
            if block_id < 0 or block_id >= innovation.size:
                continue
            if float(innovation[block_id]) <= 0.0:
                continue
            for nid in nodes_in_block:
                expert_debt[nid] = max(0, expert_debt.get(nid, 0) - 1)

    active_levels = {node_levels[nid] for nid in active_set if nid in node_levels}
    for level in set(node_levels.values()):
        band_debt.setdefault(level, 0)
    for level in list(band_debt):
        if level in active_levels:
            band_debt[level] = 0
        else:
            band_debt[level] = band_debt.get(level, 0) + 1
    valid_levels = set(node_levels.values())
    for level in list(band_debt):
        if level not in valid_levels:
            band_debt.pop(level, None)

    state.coverage_expert_debt = expert_debt
    state.coverage_band_debt = band_debt
    state.node_band_levels = node_levels


def _update_block_uncertainty(state: AgentState, sigma_diag: np.ndarray, cfg: AgentConfig) -> None:
    """Keep a cached uncertainty per block derived from the latest Sigma diagonal."""
    if sigma_diag is None:
        return
    B = int(getattr(cfg, "B", 0))
    if B <= 0:
        return
    diag = np.asarray(sigma_diag, dtype=float).reshape(-1)
    if not diag.size:
        diag = np.zeros(0, dtype=float)
    D = int(getattr(cfg, "D", diag.size))
    if diag.size != D:
        diag = np.resize(diag, (D,))

    default_unc = float(getattr(cfg, "fovea_uncertainty_default", 1.0))
    block_unc = np.zeros(B, dtype=float)
    for b in range(B):
        dims = list(dims_for_block(b, cfg))
        if not dims:
            block_unc[b] = default_unc
            continue
        idx = np.array(dims, dtype=int)
        vals = diag[idx]
        finite_vals = vals[np.isfinite(vals)]
        block_unc[b] = float(np.mean(finite_vals)) if finite_vals.size else default_unc

    state.fovea.block_uncertainty = block_unc


@dataclass
class TransportCandidate:
    """Transport candidate summary produced during evidence scoring."""

    delta: Tuple[int, int]
    shifted: np.ndarray
    mae: float
    overlap: int
    score: float
    ascii_mismatch: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass(frozen=True)
class TransportTransform:
    """Specification of a transform to evaluate (delta + rotation)."""

    delta: Tuple[int, int]
    rotation: int = 0
    label: str = "grid"


def _normalize_rotation_steps(cfg: AgentConfig) -> List[int]:
    steps = tuple(getattr(cfg, "transport_rotation_steps", (0,)))
    normalized: List[int] = []
    for step in steps:
        rot = int(step) % 4
        if rot not in normalized:
            normalized.append(rot)
    if 0 not in normalized:
        normalized.insert(0, 0)
    return normalized


def _build_transport_candidate_set(
    cfg: AgentConfig,
    env_shift: Tuple[int, int] | None,
    coarse_shift: Tuple[int, int] | None,
    state: AgentState | None,
) -> List[TransportTransform]:
    """Return a deterministic list of transport transformations to consider."""
    radius = max(1, int(getattr(cfg, "transport_search_radius", 1)))
    span = radius
    deltas: Set[Tuple[int, int]] = set()
    label_map: Dict[Tuple[int, int], str] = {}
    for dy in range(-span, span + 1):
        for dx in range(-span, span + 1):
            key = (dx, dy)
            deltas.add(key)
            label_map.setdefault(key, "grid")
    deltas.add((0, 0))
    label_map.setdefault((0, 0), "grid")

    if env_shift is not None:
        key = (int(env_shift[0]), int(env_shift[1]))
        deltas.add(key)
        label_map[key] = "env_shift"
    if coarse_shift is not None:
        key = (int(coarse_shift[0]), int(coarse_shift[1]))
        deltas.add(key)
        label_map.setdefault(key, "coarse_shift")

    offset_size = max(0, int(getattr(cfg, "transport_offset_history_size", 0)))
    offsets = []
    if state is not None and offset_size > 0:
        offsets = list(getattr(state, "transport_offsets", []))
    if offsets:
        limit = radius + max(0, int(getattr(cfg, "transport_offset_radius", 0)))
        base_deltas = list(deltas)
        for offset in offsets:
            offset_key = (int(offset[0]), int(offset[1]))
            deltas.add(offset_key)
            label_map[offset_key] = "offset"
            for base in base_deltas:
                combined = (base[0] + offset_key[0], base[1] + offset_key[1])
                if abs(combined[0]) <= limit and abs(combined[1]) <= limit:
                    deltas.add(combined)
                    label_map.setdefault(combined, "offset_combo")
            base_deltas = list(deltas)

    rotations = _normalize_rotation_steps(cfg) if bool(getattr(cfg, "transport_rotation_enabled", False)) else [0]
    transforms: List[TransportTransform] = []
    sorted_deltas = sorted(deltas, key=_transport_delta_priority)
    for delta in sorted_deltas:
        label = label_map.get(delta, "grid")
        for rot in rotations:
            transforms.append(TransportTransform(delta=delta, rotation=rot, label=label))
    return transforms


_TRANSPORT_UNINFORMATIVE_SCORE = -1e6


def _transport_candidate_score(
    mae: float,
    overlap: int,
    min_overlap: int,
    overlap_penalty: float,
    overlap_bonus: float,
) -> float:
    """Convert MAE/overlap into an evidence score (higher is better)."""
    if overlap < min_overlap or math.isinf(mae):
        return _TRANSPORT_UNINFORMATIVE_SCORE
    overlap_scale = math.log1p(float(overlap))
    normalized_mae = mae / max(overlap_scale, 1e-6)
    score = -normalized_mae
    if overlap_penalty > 0.0:
        score -= overlap_penalty / float(max(overlap, 1))
    if overlap_bonus != 0.0:
        score += overlap_bonus * math.log1p(float(overlap))
    return score


def _transport_delta_priority(delta: Tuple[int, int]) -> Tuple[int, int, int, int]:
    """Zero-bias ordering for deltas: prefer (0,0), then cardinal, then diagonals."""
    dx, dy = delta
    if dx == 0 and dy == 0:
        tier = 0
    elif dx == 0 or dy == 0:
        tier = 1
    else:
        tier = 2
    span = abs(dx) + abs(dy)
    return (tier, span, abs(dx), abs(dy))


def _logsumexp(values: Iterable[float]) -> float:
    """Numerically stable log-sum-exp for transport beliefs."""
    vals = list(values)
    if not vals:
        return float("-inf")
    max_val = max(vals)
    if math.isinf(max_val):
        return max_val
    total = 0.0
    for v in vals:
        total += math.exp(v - max_val)
    return max_val + math.log(total)


def _grid_occupancy_mask(vec: np.ndarray, cfg: AgentConfig) -> np.ndarray:
    """Return boolean occupancy mask per cell derived from a fine-grained state vector."""
    side = max(0, int(getattr(cfg, "grid_side", 0)))
    if side <= 0:
        return np.zeros(0, dtype=bool)
    cell_count = side * side
    if cell_count <= 0:
        return np.zeros(0, dtype=bool)
    channels = max(1, int(getattr(cfg, "grid_channels", 1)))
    arr = np.asarray(vec, dtype=float).reshape(-1)
    base_dim = max(0, int(getattr(cfg, "grid_base_dim", 0)))
    if base_dim <= 0:
        base_dim = int(getattr(cfg, "D", 0))
    needed = cell_count * channels
    data = np.zeros(needed, dtype=float)
    copy_len = min(needed, arr.size, base_dim)
    if copy_len > 0:
        data[:copy_len] = arr[:copy_len]
    data = data.reshape(cell_count, channels)
    return np.any(np.abs(data) > 1e-6, axis=1)
def _select_transport_delta(
    x_prev: np.ndarray,
    obs_idx: np.ndarray,
    obs_vals: np.ndarray,
    cfg: AgentConfig,
    state: AgentState,
    env_shift: Tuple[int, int] | None,
    coarse_shift: Tuple[int, int] | None,
    true_env_vec: np.ndarray | None = None,
    true_delta: Tuple[int, int] | None = None,
    force_true_delta: bool = False,
) -> Tuple[
    Tuple[int, int],
    np.ndarray,
    TransportCandidate | None,
    TransportCandidate | None,
    float,
    float,
    float,
    str,
    List[TransportCandidate],
    bool,
    float,
    float,
    bool,
    int,
]:
    """Evaluate candidate shifts, maintain belief, and return the chosen delta plus diagnostics."""
    D = _cfg_D(state, cfg)
    base_dim = max(0, int(D) - periph_block_size(cfg))
    candidate_transforms = _build_transport_candidate_set(cfg, env_shift, coarse_shift, state)

    prev_obs_dims = getattr(state.buffer, "observed_dims", set()) or set()
    prev_obs_mask = np.zeros(D, dtype=float)
    prev_buffer = np.asarray(getattr(state.buffer, "x_last", np.zeros(D)), dtype=float).reshape(-1)
    if prev_buffer.shape[0] != D:
        prev_buffer = np.resize(prev_buffer, (D,))
    prev_obs_values = np.zeros(D, dtype=float)
    for dim in prev_obs_dims:
        idx = int(dim)
        if 0 <= idx < D:
            prev_obs_mask[idx] = 1.0
            prev_obs_values[idx] = prev_buffer[idx]

    obs_mask = np.zeros(D, dtype=bool)
    obs_values = np.zeros(D, dtype=float)
    if obs_idx.size:
        obs_mask[obs_idx] = True
        obs_values[obs_idx] = obs_vals

    min_overlap = max(0, int(getattr(cfg, "transport_min_overlap", 1)))
    overlap_penalty = float(getattr(cfg, "transport_overlap_penalty", 0.0))
    overlap_bonus = float(getattr(cfg, "transport_overlap_bonus", 0.0))
    ascii_penalty = float(getattr(cfg, "transport_ascii_penalty", 0.0))
    env_occ_mask: np.ndarray | None = None
    if true_env_vec is not None:
        env_occ_mask = _grid_occupancy_mask(true_env_vec, cfg)
    candidate_infos: List[TransportCandidate] = []

    for transform in candidate_transforms:
        delta = tuple(int(v) for v in transform.delta)
        rotation = int(transform.rotation) % 4
        shifted = apply_transport(x_prev, delta, cfg, rotation=rotation)
        shifted_prev_obs = apply_transport(prev_obs_values, delta, cfg, rotation=rotation)
        mask_shifted = apply_transport(prev_obs_mask, delta, cfg, rotation=rotation)
        overlap_mask = obs_mask & (mask_shifted > 0.5)
        overlap_idx = np.nonzero(overlap_mask)[0]
        overlap = int(overlap_idx.size)
        if overlap:
            diff = obs_values[overlap_idx] - shifted_prev_obs[overlap_idx]
            mae = float(np.mean(np.abs(diff)))
        else:
            mae = float("inf")
        score = _transport_candidate_score(
            mae,
            overlap,
            min_overlap,
            overlap_penalty,
            overlap_bonus,
        )
        ascii_mismatch = 0
        if env_occ_mask is not None and env_occ_mask.size:
            cand_occ = _grid_occupancy_mask(shifted, cfg)
            if cand_occ.size > 0:
                min_len = min(env_occ_mask.size, cand_occ.size)
                mismatch = int(np.count_nonzero(env_occ_mask[:min_len] != cand_occ[:min_len]))
                mismatch += abs(env_occ_mask.size - cand_occ.size)
            else:
                mismatch = int(env_occ_mask.size)
            ascii_mismatch = mismatch
            if ascii_penalty > 0.0 and mismatch > 0:
                score -= ascii_penalty * float(mismatch)
        bias_key = (delta[0], delta[1], rotation)
        bias_bonus = float(getattr(state, "transport_biases", {}).get(bias_key, 0.0))
        weight = float(getattr(cfg, "transport_bias_weight", 0.0))
        score += bias_bonus * weight
        metadata = {
            "rotation": rotation,
            "transform_source": transform.label,
        }
        candidate_infos.append(
            TransportCandidate(
                delta=delta,
                shifted=shifted,
                mae=mae,
                overlap=overlap,
                score=score,
                ascii_mismatch=ascii_mismatch,
                metadata=metadata,
            )
        )

    beliefs = getattr(state, "transport_beliefs", {}) or {}
    decay = float(getattr(cfg, "transport_belief_decay", 0.5))
    inertia = float(getattr(cfg, "transport_inertia_weight", 0.0))
    last_delta = tuple(getattr(state, "transport_last_delta", (0, 0)))
    updated_beliefs: Dict[Tuple[int, int], float] = {}
    for cand in candidate_infos:
        prev = float(beliefs.get(cand.delta, 0.0))
        inertia_bonus = inertia if cand.delta == last_delta else 0.0
        updated_beliefs[cand.delta] = decay * prev + (1.0 - decay) * cand.score + inertia_bonus

    scores = [cand.score for cand in candidate_infos]
    if scores:
        best_score = max(scores)
        worst_score = min(scores)
    else:
        best_score = 0.0
        worst_score = 0.0
    score_spread = float(best_score - worst_score)
    sorted_by_score = sorted(candidate_infos, key=lambda c: c.score, reverse=True)
    best_raw_candidate = sorted_by_score[0] if sorted_by_score else None
    runner_raw_candidate = sorted_by_score[1] if len(sorted_by_score) > 1 else None
    runner_score = runner_raw_candidate.score if runner_raw_candidate is not None else best_score
    score_margin = float(best_score - runner_score)
    tie_threshold = float(getattr(cfg, "transport_tie_threshold", 1e-4))
    tie_flag = score_margin < tie_threshold
    uninformative_threshold = float(
        getattr(cfg, "transport_uninformative_score", _TRANSPORT_UNINFORMATIVE_SCORE)
    )
    best_informative = best_raw_candidate is not None and best_raw_candidate.score > uninformative_threshold
    evidence_margin = float(getattr(cfg, "transport_evidence_margin", 0.02))
    null_evidence = (not best_informative) or (score_margin < evidence_margin)

    if null_evidence:
        updated_beliefs = {delta: 0.0 for delta in updated_beliefs}
    state.transport_beliefs = updated_beliefs

    if not updated_beliefs:
        default = TransportCandidate(delta=(0, 0), shifted=x_prev.copy(), mae=float("inf"), overlap=0, score=_TRANSPORT_UNINFORMATIVE_SCORE)
        return (
            (0, 0),
            x_prev.copy(),
            default,
            None,
            float("inf"),
            0.0,
            0.0,
            "fallback",
            [default],
            bool(null_evidence),
            0.0,
            float(score_spread),
            bool(tie_flag),
            int(default.overlap),
        )

    log_z = _logsumexp(updated_beliefs.values())
    probs: Dict[Tuple[int, int], float] = {}
    if math.isinf(log_z):
        uniform = 1.0 / float(len(updated_beliefs))
        probs = {delta: uniform for delta in updated_beliefs}
    else:
        for delta, score in updated_beliefs.items():
            probs[delta] = math.exp(score - log_z)

    if not probs:
        probs = {(0, 0): 1.0}

    posterior_entropy = 0.0
    for p in probs.values():
        if p > 0.0:
            posterior_entropy -= p * math.log(p)

    delta_to_candidate = {cand.delta: cand for cand in candidate_infos}
    tie_prob_threshold = float(getattr(cfg, "transport_tie_probability_threshold", 1e-4))
    max_prob = max(probs.values()) if probs else 0.0
    tie_candidates = [delta for delta, prob in probs.items() if prob >= max_prob - tie_prob_threshold]
    if not tie_candidates:
        tie_candidates = list(probs.keys())

    best_delta = (0, 0) if null_evidence else min(tie_candidates, key=_transport_delta_priority)

    zero_candidate = delta_to_candidate.get((0, 0))
    if zero_candidate is None:
        zero_candidate = TransportCandidate(
            delta=(0, 0),
            shifted=x_prev.copy(),
            mae=float("inf"),
            overlap=0,
            score=_TRANSPORT_UNINFORMATIVE_SCORE,
        )

    best_candidate = delta_to_candidate.get(best_delta, zero_candidate)
    if force_true_delta and true_delta is not None:
        forced_delta = tuple(int(v) for v in true_delta)
        forced_candidate = delta_to_candidate.get(forced_delta)
        if forced_candidate is not None:
            best_delta = forced_delta
            best_candidate = forced_candidate
    x_prev_post = best_candidate.shifted.copy() if best_candidate is not None else x_prev.copy()

    sorted_probs = sorted(probs.items(), key=lambda kv: kv[1], reverse=True)
    runner_delta = next((delta for delta, _ in sorted_probs if delta != best_delta), None)
    runner_candidate = delta_to_candidate.get(runner_delta)

    best_prob = probs.get(best_delta, 0.0)
    runner_prob = probs.get(runner_delta, 0.0)
    best_score_log = updated_beliefs.get(best_delta, float("-inf"))
    runner_score_log = updated_beliefs.get(runner_delta, best_score_log)
    log_margin = best_score_log - runner_score_log
    prob_diff = best_prob - runner_prob

    source = "buffer_infer"
    if env_shift is not None:
        source = "grid"

    best_overlap = int(best_candidate.overlap) if best_candidate is not None else 0
    return (
        best_delta,
        x_prev_post,
        best_candidate,
        runner_candidate,
        log_margin,
        best_prob,
        prob_diff,
        source,
        candidate_infos,
        bool(null_evidence),
        float(posterior_entropy),
        float(score_spread),
        bool(tie_flag),
        best_overlap,
    )
def _compute_transport_disagreement_blocks(
    best: TransportCandidate | None,
    runner: TransportCandidate | None,
    cfg: AgentConfig,
) -> Dict[int, float]:
    """Compute normalized block disagreement weights for ambiguous candidates."""
    if best is None or runner is None:
        return {}
    D = int(getattr(cfg, "D", 0))
    base_dim = max(0, D - periph_block_size(cfg))
    if base_dim <= 0:
        return {}
    diff = np.abs(best.shifted[:base_dim] - runner.shifted[:base_dim])
    if not np.any(diff):
