### Elaborated Plan to Rectify Axiom Violations in NUPCA4

The following plan rectifies the identified violation of axiom A-1 (specifically A-1.2 and A-1.3) by relocating the curriculum adaptation logic from the test harness (`test4.py`) to the agent internals. This ensures the harness performs only physics simulation and action application, without heuristics based on agent cognition. The adaptation will now be an agent-sovereign decision, output as a command within the `Action` type, computed from internal prediction error streaks.

The scope of changes is strictly limited to integrating curriculum logic: no alterations to existing perception, prediction, fusion, or other core mechanisms. Modifications preserve type safety, axiom compliance (e.g., A6 budgeting, A13 ordering), and extensibility. Each step focuses on one file, building sequentially. After each, test by running `test4.py` to verify no runtime errors and inspect logs for expected behavior (e.g., streak updates in agent state).

Implement changes using Python 3 syntax, maintaining existing imports and indentation. Assume access to `numpy` (np) and `dataclasses` where needed.

#### Step 1: Modify `nupca3/types.py` (Extend Data Types for Action Commands)
   - **Scope**: Limited to the action representation section. Do not alter existing dataclasses (e.g., `EnvObs`, `Margins`, `ExpertNode`) or aliases. This step prepares type-safe command inclusion without affecting other modules.
   - **What Should Change**: Replace the alias `Action = int` with a dataclass to include a curriculum command. Add an `Enum` for commands. This changes the action from a scalar integer to a structured object, ensuring backward compatibility by defaulting to no command.
   - **Expected Input/Output**: 
     - Input: None (static types).
     - Output: Instances of `Action` will now have a `command` field (Enum value), used in downstream modules like `core.py` for action synthesis.
   - **Specific Modifications**:
     - Locate the line `Action = int` (near the top, after imports).
     - Replace it with the following:
       ```python
       from enum import Enum

       class CurriculumCommand(Enum):
           NONE = "none"
           ADD_SHAPE = "add_shape"
           REMOVE_SHAPE = "remove_shape"

       @dataclass
       class Action:
           command: CurriculumCommand = CurriculumCommand.NONE
           # Preserve scalar action if needed for other domains (stubbed as int)
           value: int = 0
       ```
     - No other changes; this ensures modules importing `Action` now get the dataclass.

#### Step 2: Modify `nupca3/step_pipeline/learning.py` (Add Streak Tracking and Residual Analysis)
   - **Scope**: Limited to adding streak computation tied to prediction residuals. Do not modify existing functions (`_feature_probe_vectors`, `_build_training_mask`, `_derive_margins`). This integrates with A13.8 (residual computation) without altering margin or probe logic.
   - **What Should Change**: Add a `LearningProcessor` class to track streaks based on `mean_delta` (mean absolute error from residuals). This will be instantiated in `core.py` and called post-residual computation.
   - **Expected Input/Output**:
     - Input: `mean_delta` (float, from residual MAE in `core.py`, typically 0.0–1.0 range).
     - Output: Updated internal streaks (integers, reset on threshold); stored in agent state for action synthesis.
   - **Specific Modifications**:
     - Append at the end of the file (after `_derive_margins`):
       ```python
       class LearningProcessor:
           def __init__(self):
               self.low_streak: int = 0
               self.high_streak: int = 0

           def update_streaks(self, mean_delta: float) -> None:
               ADD_DELTA_THRESHOLD = 0.02
               HIGH_DELTA_THRESHOLD = 0.10
               STREAK_STEPS = 20  # Matches test4.py constant

               if mean_delta < ADD_DELTA_THRESHOLD:
                   self.low_streak += 1
                   self.high_streak = 0
               elif mean_delta > HIGH_DELTA_THRESHOLD:
                   self.high_streak += 1
                   self.low_streak = 0
               else:
                   self.low_streak = 0
                   self.high_streak = 0
       ```
     - This class will be used in `core.py` to avoid global state.

#### Step 3: Modify `nupca3/step_pipeline/transport.py` (Synthesize Action Based on Streaks)
   - **Scope**: Limited to adding action synthesis for curriculum commands. Do not modify transport delta logic (e.g., `TransportCandidate`, `_select_transport_delta`). This ties into A13.10 (action synthesis) as a post-transport extension.
   - **What Should Change**: Add a function to generate `Action` using streaks from Step 2. It checks thresholds and resets streaks, ensuring commands are issued only when viable (e.g., respecting min/max shapes via later env application).
   - **Expected Input/Output**:
     - Input: `AgentState` (with streaks from `learning.py`).
     - Output: `Action` instance with `command` set if threshold met; defaults to `NONE`.
   - **Specific Modifications**:
     - Append at the end of the file (after `_select_transport_delta`):
       ```python
       from ...types import Action, CurriculumCommand

       def synthesize_action(state: AgentState) -> Action:
           STREAK_STEPS = 20
           low_streak = getattr(state, 'low_streak', 0)
           high_streak = getattr(state, 'high_streak', 0)

           action = Action()
           if low_streak >= STREAK_STEPS:
               action.command = CurriculumCommand.ADD_SHAPE
               state.low_streak = 0
               state.high_streak = 0
           elif high_streak >= STREAK_STEPS:
               action.command = CurriculumCommand.REMOVE_SHAPE
               state.low_streak = 0
               state.high_streak = 0
           return action
       ```
     - Note: Streaks are stored directly in `state` for simplicity (extend `AgentState` if needed, but avoid for minimal change).

#### Step 4: Modify `nupca3/step_pipeline/core.py` (Integrate into Pipeline Sequence)
   - **Scope**: Limited to A13 canonical ordering: add streak update after residual (A13.8) and action synthesis before output (A13.10–11). Do not alter other steps (e.g., fovea selection, transport delta, fusion).
   - **What Should Change**: Instantiate `LearningProcessor`, call `update_streaks` post-residual MAE computation, store streaks in state, and replace scalar action with synthesized `Action`.
   - **Expected Input/Output**:
     - Input: Existing `state`, `env_obs`, `cfg`.
     - Output: Updated to return `Action` instance instead of int; trace dict includes streaks for logging.
   - **Specific Modifications**:
     - Add import: `from .learning import LearningProcessor; from .transport import synthesize_action; from ...types import Action`
     - In `step_pipeline`, after `_dbg` imports, add: `learning_processor = LearningProcessor()`
     - Locate residual computation (around `abs_error = np.abs(residual)`; compute `mean_delta = float(np.mean(abs_error[obs_idx])) if obs_idx.size else 0.0`).
     - Insert after: `learning_processor.update_streaks(mean_delta); state.low_streak = learning_processor.low_streak; state.high_streak = learning_processor.high_streak`
     - Locate action selection (near end: `action = select_action(...)`).
     - Replace with: `action = synthesize_action(state)`
     - Update return: `return action, state, trace` (change type hint to `Action`).
     - In trace dict, add `'low_streak': state.low_streak, 'high_streak': state.high_streak`.

#### Step 5: Modify `test4.py` (Refactor Environment and Harness Loop)
   - **Scope**: Limited to removing curriculum heuristic and applying agent commands. Do not alter physics (`step_physics`), rendering (`render_visible`), or display logic.
   - **What Should Change**: Add `apply_action` to `SemanticOcclusionEnv` to handle commands purely as physics (e.g., call `_spawn_obj` or `_remove_one`). Remove `update_curriculum` and streaks from env; apply action in loop.
   - **Expected Input/Output**:
     - Input: `Action` from agent (with `command`).
     - Output: Updated env state (object count changes); no cognitive feedback.
   - **Specific Modifications**:
     - Add import: `from nupca3.types import Action, CurriculumCommand`
     - In `SemanticOcclusionEnv`, append:
       ```python
       def apply_action(self, action: Action):
           if action.command == CurriculumCommand.ADD_SHAPE and len(self.objs) < MAX_SHAPES:
               self._spawn_obj()
           elif action.command == CurriculumCommand.REMOVE_SHAPE and len(self.objs) > MIN_SHAPES:
               self._remove_one()
       ```
     - Remove `low_streak` and `high_streak` from `__init__`; remove `update_curriculum` method.
     - In main loop (inferred after class definitions): After `step_out = agent.step(obs)`, add `env.apply_action(step_out.action)`; remove `env.update_curriculum(step_out.mean_delta)`.
     - Ensure `agent.step` returns include `action` (from core.py changes).

#### Validation and Rollout
Post-implementation, validate axiom compliance by auditing logs: confirm curriculum changes occur via agent commands only, with no harness-side use of `mean_delta`. Run for 100+ steps to observe adaptive object counts. If errors arise (e.g., type mismatches), revert and debug sequentially. This plan ensures precise, non-tangential fixes while upholding the architecture's integrity.